# Big Data Project

## Lancement du projet

Pour lancer le projet, vous devez **d'abord cr√©er manuellement** la base de donn√©es sous la structure suivante :

```
data/
  ‚îú‚îÄ‚îÄ euronext/
  ‚îî‚îÄ‚îÄ boursorama/
```

Ensuite, vous devez **modifier** le fichier `docker-compose.yml` pour adapter le chemin dans la section `volumes` en fonction de votre machine :

```yaml
volumes:
  - /votre/chemin/vers/data:/home/bourse/data/
```

Par exemple, le chemin initial √©tait :

```yaml
volumes:
  - /mnt/c/Users/leolo/OneDrive/Bureau/temp_big_data/data:/home/bourse/data/
```

Modifiez-le pour pointer vers votre propre r√©pertoire `data`.

---

## Fonctionnalit√© suppl√©mentaire : S√©lection d'intervalle de dates

Dans le fichier `etl.py`, nous avons ajout√© une fonctionnalit√© permettant √† l'utilisateur de **choisir l'intervalle de dates** qu'il souhaite traiter, directement dans `main.py`.

Cela est utile car **calculer toutes les donn√©es de 2019 √† 2024 est tr√®s long** (~20 minutes sur un PC Cisco).  
Ainsi, si l'utilisateur souhaite par exemple uniquement les donn√©es de l'ann√©e 2019, il peut configurer comme suit :

```python
start_date = datetime(2019, 1, 1)
end_date = datetime(2019, 12, 31)
```

Cela permet de r√©duire consid√©rablement le temps de traitement.

---

## Instructions de lancement

Apr√®s avoir :

- cr√©√© la base de donn√©es (`data/euronext` et `data/boursorama`),
- modifi√© correctement le chemin dans `docker-compose.yml`,

vous pouvez **lancer le projet** en ex√©cutant la commande suivante **depuis le dossier `docker`** :

```bash
make all
```

Ensuite, **attendez que l'ETL affiche un code retour 0**, ce qui signifie que le traitement s'est termin√© avec succ√®s.  
√Ä ce moment-l√†, vous pourrez **ouvrir le tableau de bord dans votre navigateur web**.

---

## Remarques sur les performances

- Sur un PC Cisco, le traitement complet (2019-2024) prend environ **20 minutes**.
- Si vous disposez de plus ou moins de RAM, vous pouvez ajuster ce param√®tre dans `etl.py` :

```python
commit_threshold = 100000  # √Ä ajuster selon votre RAM : plus c'est √©lev√©, moins il y aura de commits, mais plus cela consommera de m√©moire.
```

---

## Optimisations Pandas et NumPy

Pour am√©liorer significativement les performances de notre pipeline ETL, nous avons impl√©ment√© plusieurs optimisations au niveau des op√©rations Pandas et NumPy :

1. **Cache et pr√©allocation m√©moire**  
   - Utilisation de caches pour les op√©rations co√ªteuses (normalisation des symboles, mappage des ID de march√©)  
   - Pr√©allocation des DataFrames NumPy pour √©viter les redimensionnements co√ªteux  

2. **Traitement par lots (Batching)**  
   - Impl√©mentation du traitement par lots pour limiter l'empreinte m√©moire  
   - Param√®tre `commit_threshold` configurable pour ajuster la taille des lots en fonction de la RAM disponible  

3. **Optimisations vectorielles Pandas**  
   - Remplacement des boucles par des op√©rations vectorielles Pandas (`groupby`, `transform`)  
   - Utilisation de types de donn√©es optimis√©s (ex : `np.int16` au lieu de `int`) pour r√©duire la consommation m√©moire  

4. **Lecture et filtrage efficace des fichiers**  
   - Filtrage des fichiers par date avant chargement complet pour √©viter les traitements inutiles  
   - Cache des dates pour √©viter les conversions r√©p√©titives  

5. **Utilisation d'algorithmes optimis√©s**  
   - Algorithme de tri efficace avec `kind='mergesort'` pour les valeurs de stocks  
   - D√©doublonnage optimis√© avec des op√©rations sur les ensembles (`set`) plut√¥t que des filtres s√©quentiels  

Ces optimisations r√©duisent consid√©rablement le temps d'ex√©cution et permettent de traiter de grands volumes de donn√©es m√™me avec des ressources limit√©es.

---
## Note

Le dashboard met du temps √† acc√©der aux donn√©es, car il doit charger toutes les donn√©es de la base de donn√©es.
Attendre 20s quand vous clicker sur Select Companies.

---
## Membres du groupes 
- L√©o Sambrook
- Arthur Hamard
- Arthur Guelennoc


N'h√©sit√© pas √† mettre 20/20 Merci!!! üöÄ 
